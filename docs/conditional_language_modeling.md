# Intuition

The main idea is using the internal model's prediction hesitation of an ASR to score the speech.

The task of _generating text_ in an ASR model is nothing more than a language model objective: next token prediction. So we can use this language model objective to get entropies. This tells us how well the model is hesitent at transcribing the input speech. __We expect the model to be less hesitant on transcribing good english speech than bad english speech.__


To do so, we give to the model an input speech and we ask it to transcribe the speech. We extract the entropies using the probability distributions generated by the model while transcribing the speech. 

# More formally

Let $\textbf{x}$ be the input speech and $\textbf{t}$ the model's transcription of this input speech.
In order to transcribe the input speech, the model chooses the next token $t_{i}$ from a probability distribution over a vocabulary $V$. This probability distribution $X^{t_{i}}$ is conditionned with the historic $h=t_{1}, t_{2}, ..., t_{i-1}$ and the input speech $x$:

```math
X^{t_{i}} = p(\cdot|t_{1}, t_{2}, ..., t_{i-1}; \textbf{x})
```

The entropy of this probability distribution can be computed as:

```math
H(X^{t_{i}}) = \sum\limits_{p\in X} p\;\times\;log\;p
```

This quantity tells us how well the model is hesitant on predicting the next token.

The overall _hesitation_ of the model at transcribing the speech can be computed as:

```math
H(\textbf{x}) = \frac{1}{n}\sum\limits_{t_{i} \in \textbf{t}} H(X^{t_{i}})
```
